import requests
import json
import time
import os
import subprocess
import csv
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½å½“å‰ç›®å½•ä¸‹çš„ .env æ–‡ä»¶
load_dotenv()

# ================= é…ç½®åŒºåŸŸ =================

# API æœåŠ¡åœ°å€
API_BASE_URL = "http://localhost:8001"

# æµ‹è¯•æ–‡ä»¶å¤¹è·¯å¾„ (ä¿®æ”¹ä¸ºæ–‡ä»¶å¤¹è·¯å¾„ï¼Œè„šæœ¬å°†è¯»å–è¯¥ç›®å½•ä¸‹æ‰€æœ‰ .txt æ–‡ä»¶)
TEST_DIR_PATH = r"D:\Personal_Project\kgplatform_backend\python-service\example"

# API é…ç½®
CONFIG = {
    "provider": "forward",
    "api_key": os.getenv("FORWARD_API_KEY"),
    "base_url": os.getenv("FORWARD_BASE_URL"),
    "model": os.getenv("FORWARD_DEFAULT_MODEL"),

    # OpenIE å¿…éœ€å‚æ•°
    "project_id": 66,
    "start_material_id": 2001,  # ä¿®æ”¹ï¼šä½œä¸ºèµ·å§‹ç´ æIDï¼Œåç»­æ–‡ä»¶è‡ªåŠ¨ç´¯åŠ 
}

# å¯¼å‡ºæ–‡ä»¶çš„ä¿å­˜ç›®å½•
EXPORT_DIR = "exports"


# ===========================================

def run_sql_export_csv(sql, output_file):
    """
    é€šè¿‡ docker exec æ‰§è¡Œ SQL å¹¶å°†ç»“æœå¯¼å‡ºä¸º CSV
    """
    # ç¡®ä¿å¯¼å‡ºç›®å½•å­˜åœ¨
    Path(EXPORT_DIR).mkdir(parents=True, exist_ok=True)
    full_path = Path(EXPORT_DIR) / output_file

    print(f"    æ­£åœ¨å¯¼å‡ºåˆ°: {full_path} ...")

    # ä½¿ç”¨ PSQL çš„ COPY ... TO STDOUT (CSV HEADER) å‘½ä»¤
    # æ³¨æ„ï¼šéœ€è¦è½¬ä¹‰ SQL ä¸­çš„åŒå¼•å·
    copy_cmd = f"COPY ({sql}) TO STDOUT WITH CSV HEADER"

    cmd = [
        "docker", "exec", "-i", "pgvector-db",
        "psql", "-U", "postgres", "-d", "kgplatform_chidu",
        "-c", copy_cmd
    ]

    try:
        with open(full_path, "w", encoding="utf-8-sig", newline="") as f:
            # æ‰§è¡Œå‘½ä»¤ï¼Œå°†æ ‡å‡†è¾“å‡ºç›´æ¥å†™å…¥æ–‡ä»¶
            result = subprocess.run(
                cmd,
                stdout=f,
                stderr=subprocess.PIPE,
                text=True,
                encoding='utf-8'  # å®¹å™¨è¾“å‡ºé€šå¸¸æ˜¯ UTF-8
            )

        if result.returncode != 0:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {result.stderr}")
        else:
            print(f"âœ… å¯¼å‡ºæˆåŠŸ: {output_file}")

    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¼‚å¸¸: {e}")


def export_project_data_to_csv(project_id):
    """
    å¯¼å‡ºæŒ‡å®šé¡¹ç›®çš„æ‰€æœ‰å®ä½“å’Œå…³ç³»åˆ° CSVï¼ŒåŒ…å«æ¥æºæ–‡ä»¶åç­‰è¯¦ç»†ä¿¡æ¯
    """
    print(f"\n{'=' * 20} å¼€å§‹å¯¼å‡ºé¡¹ç›®æ•°æ® (Project ID: {project_id}) {'=' * 20}")

    # 1. å¯¼å‡ºå®ä½“ (Nodes)
    # ä½¿ç”¨å­æŸ¥è¯¢èšåˆ source_chunk_ids å¯¹åº”çš„æ–‡ä»¶å
    nodes_sql = f"""
        SELECT 
            n.id,
            n.label,
            n.entity_type,
            n.weight,
            n.degree,
            n.description,
            -- èšåˆæ¥æº Chunk ID
            array_to_string(n.source_chunk_ids, '|') as chunk_ids,
            -- èšåˆæ¥æºæ–‡ä»¶å (ä» chunks è¡¨å…³è”)
            (
                SELECT string_agg(DISTINCT c.file_name, ' | ')
                FROM open_graph_chunks c
                WHERE c.id = ANY(n.source_chunk_ids)
            ) as source_file_names
        FROM open_graph_nodes n
        WHERE n.project_id = {project_id}
        ORDER BY n.weight DESC
    """
    run_sql_export_csv(nodes_sql, f"project_{project_id}_nodes.csv")

    # 2. å¯¼å‡ºå…³ç³» (Edges)
    edges_sql = f"""
        SELECT 
            e.id,
            s.label as source_node,
            e.relation,
            t.label as target_node,
            e.weight,
            e.description,
            -- èšåˆæ¥æº Chunk ID
            array_to_string(e.source_chunk_ids, '|') as chunk_ids,
            -- èšåˆæ¥æºæ–‡ä»¶å
            (
                SELECT string_agg(DISTINCT c.file_name, ' | ')
                FROM open_graph_chunks c
                WHERE c.id = ANY(e.source_chunk_ids)
            ) as source_file_names
        FROM open_graph_edges e
        JOIN open_graph_nodes s ON e.source_id = s.id
        JOIN open_graph_nodes t ON e.target_id = t.id
        WHERE e.project_id = {project_id}
        ORDER BY e.weight DESC
    """
    run_sql_export_csv(edges_sql, f"project_{project_id}_edges.csv")


def verify_database_content(project_id):
    """
    é€šè¿‡ Docker æ‰§è¡Œ SQL éªŒè¯æ•°æ®åº“å†…å®¹
    é‡ç‚¹æŸ¥çœ‹ï¼šæè¿°(Description)ã€æƒé‡(Weight)ã€åº¦(Degree)ã€å‘é‡(Vector)ã€æ¥æºæ–‡ä»¶(file_name)
    """
    print(f"\n{'=' * 20} æ•°æ®åº“å­˜å‚¨éªŒè¯ (Project ID: {project_id}) {'=' * 20}")

    # å®šä¹‰æŸ¥è¯¢
    queries = [
        {
            "name": "1. åˆ†å—è¡¨ (open_graph_chunks) - æ£€æŸ¥åŸæ–‡ã€å‘é‡å’Œæ–‡ä»¶å",
            "sql": f"""
                SELECT chunk_index, 
                       file_name,            -- æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦æ­£ç¡®å­˜å…¥
                       left(id, 20) as chunk_id_prefix, 
                       left(text_content, 30) as content_preview, 
                       (embedding IS NOT NULL) as has_vector 
                FROM open_graph_chunks 
                WHERE project_id = {project_id} 
                ORDER BY file_id, chunk_index ASC LIMIT 5;
            """
        },
        {
            "name": "2. å®ä½“è¡¨ (open_graph_nodes) - æ£€æŸ¥ã€æè¿°ã€‘ã€ã€æƒé‡ã€‘ã€ã€åº¦ã€‘",
            "sql": f"""
                SELECT id, 
                       label, 
                       weight, 
                       degree, 
                       left(description, 30) as desc_preview, 
                       (embedding IS NOT NULL) as has_vec 
                FROM open_graph_nodes 
                WHERE project_id = {project_id} 
                ORDER BY weight DESC LIMIT 5;
            """
        }
    ]

    for q in queries:
        print(f"\n--- {q['name']} ---")
        cmd = [
            "docker", "exec", "-i", "pgvector-db",
            "psql", "-U", "postgres", "-d", "kgplatform_chidu",
            "-c", q['sql']
        ]

        try:
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='replace')
            if result.returncode == 0:
                print(result.stdout)
            else:
                print(f"âŒ æŸ¥è¯¢å¤±è´¥: {result.stderr}")
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {e}")


def test_open_ie_task():
    # 1. æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    dir_path = Path(TEST_DIR_PATH)
    if not dir_path.exists() or not dir_path.is_dir():
        print(f"âŒ é”™è¯¯ï¼šæµ‹è¯•ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯æ–‡ä»¶å¤¹ -> {TEST_DIR_PATH}")
        return

    # 2. è·å–ç›®å½•ä¸‹æ‰€æœ‰ .txt æ–‡ä»¶
    txt_files = list(dir_path.glob("*.txt"))
    if not txt_files:
        print(f"âŒ é”™è¯¯ï¼šåœ¨ç›®å½•ä¸­æœªæ‰¾åˆ° .txt æ–‡ä»¶ -> {TEST_DIR_PATH}")
        return

    print(f"ğŸ“‚ æ‰¾åˆ° {len(txt_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå‡†å¤‡æ‰¹é‡å¤„ç†...")

    # 3. æ„é€  files åˆ—è¡¨
    files_payload = []
    start_id = CONFIG["start_material_id"]

    for idx, file_obj in enumerate(txt_files):
        # ä½¿ç”¨ç»å¯¹è·¯å¾„
        abs_path = str(file_obj.resolve())
        # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ†é…é€’å¢çš„ material_id
        current_material_id = start_id + idx

        files_payload.append({
            "material_id": current_material_id,
            "url": abs_path
        })
        print(f"  - [{current_material_id}] {file_obj.name}")

    # 4. æ„é€ è¯·æ±‚ Payload
    url = f"{API_BASE_URL}/api/v1/tasks"

    payload = {
        "task_type": "open_ie",
        "project_id": CONFIG["project_id"],
        "provider": CONFIG["provider"],
        "api_key": CONFIG["api_key"],
        "model": CONFIG.get("model"),
        "base_url": CONFIG.get("base_url"),
        "files": files_payload  # ä¼ å…¥æ–‡ä»¶åˆ—è¡¨
    }

    print(f"\n[1] æ­£åœ¨æäº¤ OpenIE ä»»åŠ¡ (æ‰¹é‡æ¨¡å¼)...")
    print(f"    æ–‡ä»¶æ•°é‡: {len(files_payload)}")
    print(f"    é¡¹ç›®ID: {CONFIG['project_id']}")

    try:
        # 5. å‘é€åˆ›å»ºä»»åŠ¡è¯·æ±‚
        response = requests.post(url, json=payload)

        if response.status_code != 201:
            print(f"âŒ åˆ›å»ºå¤±è´¥ (Status {response.status_code}):")
            print(response.text)
            return

        data = response.json()
        task_id = data.get("task_id")
        print(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ! Task ID: {task_id}")

        # 6. è½®è¯¢ç›‘æ§è¿›åº¦
        print(f"\n[2] å¼€å§‹ç›‘æ§ä»»åŠ¡è¿›åº¦...")
        start_time = time.time()

        while True:
            status_url = f"{API_BASE_URL}/api/v1/tasks/{task_id}"
            status_resp = requests.get(status_url)

            if status_resp.status_code != 200:
                print(f"    è·å–çŠ¶æ€å¤±è´¥: {status_resp.text}")
                break

            status_data = status_resp.json()
            state = status_data.get("status")
            processed = status_data.get("processed_files", 0)
            total = status_data.get("total_files", 0)
            total_triples = status_data.get("total_triples", 0)

            elapsed = int(time.time() - start_time)
            print(f"\r    >> [{elapsed}s] çŠ¶æ€: {state} | è¿›åº¦: {processed}/{total} | å·²æŠ½å–: {total_triples}", end="")

            if state in ["completed", "failed", "cancelled"]:
                print(f"\n\n[3] ä»»åŠ¡ç»“æŸ. æœ€ç»ˆçŠ¶æ€: {state}")

                if state == "completed":
                    # === éªŒè¯æ•°æ®åº“ ===
                    verify_database_content(CONFIG["project_id"])

                    # === å¯¼å‡º CSV ===
                    export_project_data_to_csv(CONFIG["project_id"])

                if state == "failed":
                    errors = status_data.get("errors", [])
                    print("âŒ é”™è¯¯ä¿¡æ¯:")
                    print(json.dumps(errors, indent=2, ensure_ascii=False))
                break

            time.sleep(2)

    except requests.exceptions.ConnectionError:
        print(f"\nâŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ {API_BASE_URL}ï¼Œè¯·ç¡®è®¤ python-service æ˜¯å¦å·²å¯åŠ¨ã€‚")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿå¼‚å¸¸: {str(e)}")


if __name__ == "__main__":
    test_open_ie_task()